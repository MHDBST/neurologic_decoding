{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "\n",
    "# input = \"My name is Azeem and I live in India\"\n",
    "# input = \"It has never been easy to have a rational conversation about the value of gold.\"\n",
    "input = \"The horse rides the astronaut. \"\n",
    "# You can also use \"translate English to French\" and \"translate English to Romanian\"\n",
    "input_ids = tokenizer(\"translate English to German: \"+input, return_tensors=\"pt\").input_ids  # Batch size 1\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "\n",
    "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert SGM to txt file\n",
    "import re\n",
    "path='./sgm/newstest2019-ende-src.en.sgm'#'./sgm/newstest2019-ende-ref.de.sgm'#'./sgm/newstest2019-deen-ref.en.sgm'\n",
    "text_out='newstest2019-ende-src.txt'#'newstest2019-ende-ref.txt'#'newstest2019-deen-ref.txt'\n",
    "seg_re = re.compile(r\"<seg id=\\\"\\d+\\\">(.*)</seg>\")\n",
    "with open(text_out,'w', encoding=\"utf-8\") as g:\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            seg_match = re.match(seg_re, line)\n",
    "            if seg_match:\n",
    "                assert len(seg_match.groups()) == 1\n",
    "            else:\n",
    "                continue\n",
    "            out_line= seg_match.groups()[0]\n",
    "            g.write(out_line)\n",
    "            g.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Translate txt from de to en\n",
    "# from transformers import FSMTForConditionalGeneration, FSMTTokenizer, \n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "mname = 'Helsinki-NLP/opus-mt-de-en'#\"facebook/wmt19-en-de\"#\"facebook/wmt19-de-en\"\n",
    "# tokenizer = FSMTTokenizer.from_pretrained(mname)\n",
    "# model = FSMTForConditionalGeneration.from_pretrained(mname)\n",
    "\n",
    "text_out='newstest2019-deen-in.txt'\n",
    "import torch\n",
    "with torch.no_grad():\n",
    "    model=MarianMTModel.from_pretrained(mname).to('cuda')\n",
    "tokenizer=MarianTokenizer.from_pretrained(mname)\n",
    "fair_out_path = 'newstest2019-deen-helsinki_out.txt'\n",
    "with open(text_out, encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "with open(fair_out_path,'w', encoding=\"utf-8\") as g:\n",
    "    for input in lines:\n",
    "        input_ids = tokenizer.encode(input.strip(), return_tensors=\"pt\").to('cuda')\n",
    "        outputs = model.generate(input_ids)\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # print(decoded)\n",
    "        g.write(decoded)\n",
    "        g.write('\\n')\n",
    "        #  # Machine learning is great, isn't it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "newstest2019-deen-helsinki_out.txt\n",
      "720\n",
      "reslts: BLEU score: 40.08618133755379\n",
      "SD_out_deen_v5_alltags_beam10_SAT3.out\n",
      "640\n",
      "reslts: BLEU score: 38.36683188368516\n",
      "SD_out_deen_v4_lemma_beam5_SAT2_prune100.out\n",
      "720\n",
      "reslts: BLEU score: 38.22663389109872\n",
      "SD_out_deen_v4.out\n",
      "128\n",
      "reslts: BLEU score: 34.718709113680596\n",
      "newstest2019-deen-out.txt\n",
      "720\n",
      "reslts: BLEU score: 40.48188167528294\n",
      "SD_out_deen_neurologic_deocding.out\n",
      "720\n",
      "reslts: BLEU score: 40.06757138362206\n"
     ]
    }
   ],
   "source": [
    "## evaluate the output\n",
    "from sacrebleu import corpus_bleu\n",
    "def calculate_bleu_score(output_lns, refs_lns, score_path):\n",
    "    bleu = corpus_bleu(output_lns, [refs_lns])\n",
    "    result = \"BLEU score: {}\".format(bleu.score)\n",
    "    print('reslts:',result)\n",
    "    with open(score_path,\"w\") as score_file:\n",
    "        score_file.write(result)\n",
    "all_paths=['newstest2019-deen-helsinki_out.txt','SD_out_deen_v5_alltags_beam10_SAT3.out',\\\n",
    "    'SD_out_deen_v4_lemma_beam5_SAT2_prune100.out','SD_out_deen_v4.out', \\\n",
    "        'newstest2019-deen-out.txt', 'SD_out_deen_neurologic_deocding.out' ]        \n",
    "for item in all_paths:\n",
    "    print(item)\n",
    "    out_path =item\n",
    "    #'SD_out_deen_neurologic_deocding.out' ,  \n",
    "    with open(out_path) as output:\n",
    "        output_lns=output.readlines()[:1000]\n",
    "    print(len(output_lns))\n",
    "    with open('newstest2019-deen-ref.txt') as reflns:\n",
    "        refs_lns=reflns.readlines()[:1000]\n",
    "    #('newstest2019-deen-out.txt','newstest2019-deen-ref.txt') -> 41.17\n",
    "    #('newstest2019-deen-out.txt','newstest2019-deen-ref.txt')  -> 39.26\n",
    "    calculate_bleu_score(output_lns, refs_lns, './blueScore_%s'%out_path.split('.')[:-1])\n",
    "\n",
    "        # output_lns1=[]\n",
    "        # refs_lns1=[]\n",
    "#         for i,line1 in enumerate(output_lns):\n",
    "#             line2 = refs_lns[i]\n",
    "# #     if len(line2.split('.')) > 2:\n",
    "# #         print(i)\n",
    "# #         continue\n",
    "# #     # if line1.split() < line2.split():\n",
    "# #     #     continue\n",
    "#         output_lns1.append(line1)\n",
    "#         refs_lns1.append(line2)\n",
    "    # print('data length is:',i+1)   \n",
    "    # print('limited data length is:',len(output_lns1))       \n",
    "    # calculate_bleu_score(output_lns1, refs_lns1, './blueScore_SD_5')\n",
    "    \n",
    "# newstest2019-deen-helsinki_out.txt\n",
    "# reslts: BLEU score: 39.508582858290566\n",
    "# SD_out_deen_v3.out\n",
    "# reslts: BLEU score: 34.84768488447905\n",
    "# SD_out_deen_v4.out\n",
    "# reslts: BLEU score: 34.718709113680596\n",
    "# newstest2019-deen-out.txt\n",
    "# reslts: BLEU score: 41.16915939988038\n",
    "# SD_out_deen_neurologic_deocding.out\n",
    "# reslts: BLEU score: 39.62539180524859\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mbastan/anaconda3/envs/hug3/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-01-20 01:02:01 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 62.6MB/s]                    \n",
      "2023-01-20 01:02:01 WARNING: Language de package default expects mwt, which has been added\n",
      "2023-01-20 01:02:02 INFO: Loading these models for language: de (German):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "=======================\n",
      "\n",
      "2023-01-20 01:02:02 INFO: Use device: gpu\n",
      "2023-01-20 01:02:02 INFO: Loading: tokenize\n",
      "2023-01-20 01:02:03 INFO: Loading: mwt\n",
      "2023-01-20 01:02:03 INFO: Loading: pos\n",
      "2023-01-20 01:02:04 INFO: Loading: lemma\n",
      "2023-01-20 01:02:04 INFO: Loading: depparse\n",
      "2023-01-20 01:02:04 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "## extract structures\n",
    "import stanza\n",
    "from typing import List\n",
    "from stanza_batch import batch\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "path1='newstest2019-deen-in.txt'\n",
    "# path1='/home/mbastan/context_home/structuralDecoding/a_star_neurologic/dataset/machine_translation/newstest2017-iate/iate.414.terminology.tsv.de'\n",
    "\n",
    "pkname='newstest2019'\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "nlp = stanza.Pipeline('de', processors='lemma,tokenize,pos,depparse',use_gpu=True)\n",
    "with open(path1) as f:\n",
    "    lines = f.readlines()\n",
    "try:\n",
    "    with open('stanza_documents_%s_de.pk'%pkname,'rb') as f:\n",
    "        stanza_documents=pickle.load(f)\n",
    "except:\n",
    "    ## batch document processing to increase the speed\n",
    "    stanza_documents: List[stanza.Document] = []\n",
    "    for document in batch(lines, nlp, batch_size=32): # Default batch size is 32\n",
    "        stanza_documents.append(document)\n",
    "    ## save the documents on a file\n",
    "    with open('stanza_documents_%s_de.pk'%pkname,'wb') as f:\n",
    "        pickle.dump(stanza_documents,f)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-20 01:52:14 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 66.7MB/s]                    \n",
      "2023-01-20 01:52:15 INFO: Loading these models for language: en (English):\n",
      "=======================================\n",
      "| Processor | Package                 |\n",
      "---------------------------------------\n",
      "| tokenize  | combined                |\n",
      "| pos       | combined                |\n",
      "| lemma     | combined                |\n",
      "| depparse  | /home/mbas..._parser.pt |\n",
      "=======================================\n",
      "\n",
      "2023-01-20 01:52:15 INFO: Use device: gpu\n",
      "2023-01-20 01:52:15 INFO: Loading: tokenize\n",
      "2023-01-20 01:52:15 INFO: Loading: pos\n",
      "2023-01-20 01:52:15 INFO: Loading: lemma\n",
      "2023-01-20 01:52:15 INFO: Loading: depparse\n",
      "2023-01-20 01:52:16 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing 100\n",
      "processing 200\n",
      "processing 300\n",
      "processing 400\n",
      "processing 500\n",
      "processing 600\n",
      "processing 700\n",
      "processing 800\n",
      "processing 900\n",
      "processing 1000\n",
      "processing 1100\n",
      "processing 1200\n",
      "processing 1300\n",
      "processing 1400\n",
      "processing 1500\n",
      "processing 1600\n",
      "processing 1700\n",
      "processing 1800\n",
      "processing 1900\n",
      "processing 2000\n"
     ]
    }
   ],
   "source": [
    "# from deep_translator import PonsTranslator\n",
    "# translator = PonsTranslator(source='de', target='en')\n",
    "nlp_en = stanza.Pipeline('en', processors='lemma,tokenize,pos,depparse',use_gpu=True,\\\n",
    "    depparse_model_path='/home/mbastan/context_home/structuralDecoding/stanza_model/en_outall_parser.pt')\n",
    "# exit()\n",
    "\n",
    "# from deep_translator import GoogleTranslator\n",
    "# from deep_translator import LingueeTranslator\n",
    "from googletrans import Translator\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# translator = LingueeTranslator(source='de', target='en')\n",
    "translator = Translator()\n",
    "\n",
    "\n",
    "outpath='newstest2019_deen_en_v4_lemma.constraint.json' #'iate.414_en.terminology.constraint.json'\n",
    "with open(outpath,'w') as f:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    german_stop_words=set(stopwords.words('german'))\n",
    "    \n",
    "    indd = 0\n",
    "    for doc in  stanza_documents:\n",
    "        indd += 1\n",
    "        if indd % 100==0:\n",
    "            print('processing %d'%indd)\n",
    "        sent = doc.sentences[0]\n",
    "        arr_all = []\n",
    "        arr_ori = []\n",
    "        arr_rev = []\n",
    "        arr_lex =[]\n",
    "        sent_map ={}\n",
    "        for word in sent.words:\n",
    "                # print('word is',word.text)\n",
    "                if not any(c.isalpha() for c in word.text):\n",
    "                    continue\n",
    "                arr_all.append([word.text,word.deprel])\n",
    "                if not (word.deprel == 'nsubj' or word.deprel == 'obj' or word.deprel == 'root' or word.deprel == 'obl'):\n",
    "                    continue\n",
    "                translated_words=[]\n",
    "                try:\n",
    "                    # translated_words=translator.translate(word.text,return_all=True)\n",
    "                    tr=translator.translate(src='de', dest='en',text=word.text)\n",
    "                    # print('tr is',tr)\n",
    "                    translated_words.append(tr.text)\n",
    "                    if tr.extra_data['all-translations']:\n",
    "                        translated_words.extend(tr.extra_data['all-translations'][0][1])\n",
    "                    # print('translated_words',translated_words)\n",
    "                except Exception as e:\n",
    "                    print('error e:',e)\n",
    "                    translated_words=[word.text]\n",
    "                # for translated_word in translated_words:\n",
    "                translated_nlp=nlp_en('\\n'.join(item for item in translated_words))\n",
    "                translated_lemma=[]\n",
    "                for translated_sent in translated_nlp.sentences:\n",
    "                    for w in translated_sent.words:\n",
    "                        if w.lemma:\n",
    "                            curr_w=w.lemma\n",
    "                        else:\n",
    "                            curr_w=w.text\n",
    "                        if not(curr_w in stop_words or curr_w.lower() in stop_words):\n",
    "                            translated_lemma.append(curr_w)\n",
    "                            translated_lemma.append(curr_w.lower())\n",
    "                # print('translated_lemma',translated_lemma)             \n",
    "                for tlemma in set(translated_lemma):  \n",
    "                    arr_ori.append([tlemma,word.deprel])\n",
    "                    \n",
    "        json.dump(arr_ori,f)\n",
    "        f.write('\\n')\n",
    "        f.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Munich woman'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # nlp_en('\\n'.join(item for item in translated_words))\n",
    "# # translated_words\n",
    "# # translator.translate(word.text)\n",
    "# from deep_translator import LingueeTranslator, GoogleTranslator\n",
    "\n",
    "# word = 'good'\n",
    "\n",
    "# GoogleTranslator(source='de', target='en').translate('Münchnerin')\n",
    "\n",
    "# # word\n",
    "# tr.text()\n",
    "# word\n",
    "# translator.translate(src='de', dest='en',text='Schöne').text\n",
    "tr.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "translator = Translator()\n",
    "word='werden'\n",
    "st=translator.translate(text='Münchnerin',src='de',dest='en')#.extra_data['all-translations'])\n",
    "st.extra_data['all-translations']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punct\n",
      "\"\n",
      "nashod: error: Request exception can happen due to an api connection error. Please check your connection and try again\n",
      "\"\n",
      "amod\n",
      "Schöne\n",
      "all transltions ['beauty ', 'Beauty and the Beast ', 'to look after sth ', 'to take care of sth ', 'to be kind to sth ', 'to go easy on sth ', \"to be kinder to one's liver \", \"to have to go bit easy on one's health heart \", 'it is easy on the joints ', 'to be kind to sth ', 'this detergent is kind to the fabric your hands ', 'to spare sb sth ', \"to spare sb's feelings \", 'to spare sb ', 'to take things easy ', 'Scania ', 'to embellish sth ', 'to dress sth up ', 'to refine or clear sth ', 'are we there yet ', 'have you heard ', \"they're coming today \", 'you want to leave now or already ', 'was finished after only five minutes ', 'was already finished after five minutes ', 'very soon after ', 'it is already late or late already ', \"just one minute and then I'm gone \", 'hardly have time to lie down before the telephone rings yet again ', 'she had hardly left the room when he lit up ', 'you can start now ', \"what o'clock already \", 'he now weighs 200 kg ', 'just or even small amount can work wonders ', 'even partial success would be worth it ', 'blank CDs can be had for as little as few cents ', 'children as young as eight are surfing the internet ', 'there were microcomputers as early as or back in the seventies ', 'that that idea is as old as Ovid ', \"since was child I've always wanted to be an actor \", 'even at that time even yesterday even now ', 'early on ', 'always ', 'she always was difficult ', \"I've always said so \", 'for years now ', 'for long time now ', 'for ages ', 'ages ago ', 'several times already ', 'know that already ', 'wanted to ask anyway ', 'ever ', 'have you ever eaten oysters ', 'as was has been said already or beforehand ', 'as was has been often the case before ', \"why complain we're badly off as it is \", 'for that reason alone ', 'the fact alone that ', 'the very fact that ', 'to be already reason enough ', 'that really is disgrace ', \"it's true all right \", 'can see ', 'can well imagine what you want ', 'those are promising words ', \"you'll see \", 'it really is pleasant doing nothing ', \"don't want it and especially not from you \", 'once again ', 'not that her again ', 'hurry up ', 'will you stop that ', 'out with it ', 'go on ', 'come on give it here ', 'hurry up ', \"if you really can't stay then have bite to eat \", 'if do drive you then before midnight ', 'all right ', 'thanks can manage ', 'it will work out all right ', 'in the end ', 'one day ', 'it will work out in the end or one day ', 'think so ', 'such things can happen ', \"do feel like it simply don't have time \", \"that's true enough \", \"that's possible true suppose \", \"that's possible but not very probable \", 'well yes or sure but ', \"do you think she's attractive yes she's okay \", 'okay ', 'so what ', \"we weren't allowed to play outside but the others were \", \"you won't get to Cologne on this road but on that one \", \"what's wealth of all things \", 'what have you of all people got to say ', 'what do ten years matter ', 'what does it matter ', 'what possible use is that ', \"who's possibly interested \", 'the mere smell sight of that ', 'just hearing about it ', 'what was your name again ', 'possible ', 'everything possible ', 'he tried everything imaginable to ', 'the only option open to us etc or thing we etc can do ', 'to believe in sth ', 'to think it possible that ', \"to do everything in one's power or utmost \", 'all kinds or sorts of ', 'maybe ', 'possibly ', 'that may well be ', 'potential ', 'possible ', 'is this really possible ', 'it is possible that ', 'sb is able to or can or it is possible for sb to do sth ', \"come with us if you're able to \", 'to make sth possible for sb ', 'to make it possible to do sth ', 'if at all possible ', \"that's impossible \", \"don't believe it \", 'as as possible ', 'come as quickly as possible ', 'yes ', \"is someone there yes it's me \", 'yes hallo ', 'is that Prof Schlüter speaking yes hallo ', \"just moment yes what is it you've dropped something \", \"that's exactly what say \", \"that's exactly what I've been saying the whole time \", 'to say yes to sth ', 'to agree to sth ', 'yes of course ', \"are you coming to Wilhelm's party yes of course \", 'yes thank you ', 'would you like some chocolate yes thanks ', 'yes or no ', \"I've had bellyful I'm handing in my notice really \", 'really ', \"I'm emigrating really \", 'make sure you arrive on time ', 'do be careful with the knife ', \"don't go there whatever you do \", \"and don't you dare tell me that \", \"don't you ever do that \", \"don't cry after all it's not that bad \", 'can try it of course ', \"that's certainly true but we should be more careful anyhow \", \"would love to but can't \", 'have to recognize that even praise it in fact ', 'it is difficult or even impossible ', 'you really are crafty devil ', \"what did tell you I've always said that you know \", 'it just had to turn out like that ', \"we've been waiting for you the whole time you know \", \"where's the damned key oh that's where it's got to \", 'admittedly ', 'certainly ', 'to be sure ', 'oh yes ', \"“that's how it was in those days do you remember “oh yes \", \"what you're telling me certainly is scarcely believable \", \"that's just terrible \", 'understand that admittedly ', \"I'm coming \", \"that's the absolute limit \", \"it's always the same you know \", \"you know him don't you \", 'yes those were the days ', \"well if that's the case I'll surely come with you \", \"well you don't say \", \"our agreement does stand though doesn't it \", \"but you'll stand by me won't you \", \"I'm sure don't know how I'm going to get him to understand that \", \"all right all right I'm coming \", 'yes all right ', 'go on go on ', 'go on go on let him have it ', \"to give sth one's blessing \", \"if the management gives it's blessing to the plan we can get going \", 'well ', \"how's the food well not bad at all really \", 'if so ', \"hopefully that won't apply if it does we'll have few more problems \", 'alone ', 'to leave sb alone ', \"we're on our own at last \", 'are you by yourself or with someone ', 'lonely ', \"on one's own \", \"to be on one's own \", \"to be left to one's own resources \", 'by oneself ', \"on one's own \", 'he prefers to work alone ', 'in itself ', 'this incident is in itself serious enough ', 'just ', 'the extent of the damage alone was bad enough ', 'the mere or very thought of it ', 'exclusively ', \"that's up to you \", \"it's your decision and yours alone \", 'the only true church teaching ', 'exclusively entitled ', 'to have the exclusive right to do sth ', 'single-handedly ', \"on one's own \", 'by oneself ', 'our youngest can already dress himself or get dressed by himself ', \"to bring up or raise child on one's own \", 'single mother single father ', 'to be single parent ', 'by itself oneself ', 'would have thought of it myself ', 'unaccompanied ', 'alone ', 'the house is completely isolated ', 'not only or just but also ', 'beautiful ', 'handsome ', 'she has beautiful lovely or nice figure ', 'what would you like my pretty one ', 'the fair er sex dated ', 'lovely ', 'nice ', 'he brought her something nice ', 'sb finds sth beautiful lovely or nice ', 'sth appeals to sb ', 'beauty ', 'beautiful things ', 'the rich and the beautiful ', 'great ', 'lovely ', 'splendid ', 'wonderful ', 'good ', 'pleasant ', 'fine ', 'nice ', 'those were wonderful days ', 'have good or nice holiday day ', 'have good or nice weekend ', 'the weather is fine here ', 'can think of better things to do than just sitting here ', 'it was all rather or quite unpleasant for him ', 'enjoy yourselves for week ', 'everything was perfect or in perfect order ', 'really thought everything was going to be fine but ', \"that's getting beyond joke \", 'you were quite rude to her ', 'the nice thing about sb sth ', 'wonderfully ', 'marvellously or ', 'the cake was brilliant success ', \"it's good that \", \"I'm pleased that \", 'there is or could be nothing nicer than ', 'nothing beats cold beer ', \"there's nothing more pleasant than sitting in the garden in summer \", 'to be nice to be not very nice of sb ', 'to die peacefully ', \"that's or how nice also \", \"you don't say so \", 'fine words ', \"that's too good to be true \", 'pleasant or nice trait ', \"that's one of the good or pleasant or nice things about her \", 'good ', 'to smell taste really or esp real good ', 'thank you very much for sth ', 'best wishes ', 'Uli sends you his kind regards ', 'all right or okay then ', 'fine ', \"that's all very well but \", 'that may well be but ', 'great ', 'good ', 'whopping ', 'handsome ', 'tidy ', \"it's quite size \", 'to reach ripe or fine old age ', 'no mean feat ', 'quite an achievement ', 'real fright ', 'quite fright ', 'quite bit of work quite stretch ', 'nice bit of cash ', 'great ', 'nice ', 'what wonderful or great prospects ', 'things are getting worse and worse ', 'fine mess ', 'the best of it ', 'the worst thing about it is that ', 'the worst of it is that ', 'astonishing ', 'beautifully ', 'they have beautiful or lovely home ', 'to get dressed or dolled up ', \"to make oneself or get dolled up do one's hair nicely \", 'well ', 'to have good time lie-in break rest ', 'to live well somewhere ', 'to have good or nice time somewhere ', 'well ', 'nicely ', 'you did that well or nicely ', 'you did or made good job of it ', 'as they say ', 'as they say ', 'everybody get in nice orderly line ', \"be good and don't get up \", 'listen carefully ', \"now clear away your toys there's good boy girl \", 'nice and big cold slow sweet ', 'be good boy girl ', 'really ', 'that really hurt ', 'piece ', 'piece or slice of bread cake ', 'piece of paper ', 'scrap of paper ', 'six pieces or portions of cheesecake ', 'bar of soap ', 'lump of sugar ', 'in one piece ', 'to be made from one or single piece ', 'piece by piece ', 'bit by bit ', 'in one piece ', 'sliced or unsliced ', 'piece ', 'item ', 'unit ', \"I'll take three of those apples over there \", 'by the piece ', 'to be paid on piece-work basis ', 'each ', 'four euros each ', 'piece ', 'item ', 'rare specimen ', 'valuable item ', 'bit ', 'piece ', 'to break or shatter into pieces ', 'to tear sth to pieces or shreds ', 'to smash sth to pieces ', 'to smash to smithereens ', 'part ', 'passage ', \"I'll come part of the way with you \", '500 metre stretch of the road had been ripped up ', 'part of field ', 'my bit of garden ', 'chapter of history ', 'plot of land ', 'in 50 euro notes ', 'play ', 'piece ', 'so-and-so ', 'you rotten or lousy bastard ', \"she's really nasty piece of work \", 'piece of shit ', 'bastard ', 'bitch ', 'in all many respects ', \"to take leaf out of sb's book \", 'job ', 'tough job ', 'quite job ', \"sb's pride and joy \", \"the apple of sb's eye \", \"of one's own free will \", 'voluntarily ', 'to think highly or the world of sb ', 'good or fair bit ', 'to get good bit further or make considerable progress ', 'pretty penny ', 'incessantly ', 'not bit ', 'not at all ', 'to do anything or go through fire and water for sb ', 'to rather die than ', 'to be bit much or thick ', 'little bit or bit ', 'somewhat ', 'pole ', 'rod ', 'bar ', 'stick ', 'carton ', 'barre ', 'perch ', 'roost ', 'tall glass ', 'beam ', 'bit ', 'rod ', 'hard-on ', 'to stick at it ', 'pretty penny ', 'packet ', 'that must have cost you them etc pretty penny or packet ', 'to stand or stick up for sb ', 'to keep sb at it ', 'off the peg or rack ', 'to buy clothes off the peg ', 'to buy off-the-peg or ready-to-wear clothes ', 'literature no no ', 'the belles-lettres ', 'literature no no ', 'tiled stove fitted with stove bench ', 'art ', 'abstract art ', 'graphic art ', 'the fine arts ', 'art ', 'art ', 'skill ', \"that's an art in itself \", 'black magic ', 'to be unprofitable ', \"there's no money in poetry \", 'to be at total loss ', \"to try one's hand at sth \", \"that's all there is to it \", \"how's it going \", 'how are tricks ', 'to be easy or simple or nothing ', 'all ', 'whole ', 'entire ', 'he dedicated all his energy to the project ', \"it's been raining all or the whole or the entire day \", \"are those all the CDs you've got \", 'the whole of or all Berlin looked on as the last piece of the wall was removed ', 'all that or the entire rubbish ended up on the scrap heap ', 'all that fuss over woman ', 'all the work ', 'the whole of Germany England ', 'this regulation applies throughout or to the whole of Europe ', 'we travelled all over Italy ', 'real man ', 'the whole truth ', 'all the time ', 'the whole time ', 'all ', 'all the cars in our street where damaged ', 'where did all these people suddenly come from ', 'all my 500 euros were stolen ', 'complete turn ', 'semibreve ', 'whole note ', 'whole number ', 'integer ', 'quite lot while ', 'intact ', 'hope our good glasses are still in one piece ', 'she only gave me her broken toys and kept the intact ones ', 'to mend sth ', 'to be mended ', 'the car has been repaired ', 'all of ', 'no more than ', 'the television cost all of 50 euros ', 'she earns all of 200 euros month ', \"it didn't take him more than ten minutes \", \"all got for five hours' heavy work was 50 euros \", 'really ', 'very ', 'that was really kind of you ', 'he said something really stupid ', \"he's really shrewd one \", \"you've made really good job of that \", \"you've made really good job of this cake \", \"are you sure you're telling the whole truth \", 'particularly ', 'especially ', 'that was particularly careless of you ', 'just little bit ', 'quite ', 'earn quite good salary really ', 'the proposal is quite interesting ', 'she quite liked it here ', 'it took quite time ', 'the children were pretty dirty ', \"you don't have to colour or in all the picture \", \"didn't see all the film \", 'have you painted all the wall ', 'have you finished painting the wall ', 'to read sth from cover to cover ', \"haven't finished reading the magazine yet \", 'completely ', 'totally ', 'to be all wet ', 'to be completely or totally covered in mud ', 'completely ', 'totally ', 'he is just like his father ', 'she was all attention ', 'quite agree ', 'just as you think best wish ', 'to be all alone ', 'completely ', 'utterly ', 'that is something completely or totally different ', 'not at all ', 'not in the least ', 'to do sth properly or not at all ', 'definitely ', 'no matter ', 'no matter what happens stay with you ', 'must have this car no matter what it costs ', 'to be all the same to sb ', \"it's all the same to me \", 'not quite ', 'it is not quite midnight yet ', 'he is just under eighteen ', \"that's not quite the same thing \", 'to be quite or absolutely right ', 'right at the back front ', 'thank you ', 'thanks ', 'ta ', 'no thank you or thanks ', \"how's it going can't complain \", 'can help thanks but think can manage ', 'no more for me thank you ', 'thank you for calling ', 'thank you for letting me know ', 'thank you for having me ', 'to say thank you to sb for sth for doing sth ', 'thank you or thanks very much ', 'thanks million ', 'yes please ', 'yes thank you ', 'no thank you or thanks ', 'please ', 'what can do for you ', 'can help you ', 'no please ', \"please don't \", 'hello ', 'yes ', 'please hold the line ', 'waiter could have the bill please ', 'this way please ', 'after you ', 'please take seat ', 'come in ', \"won't you please \", 'one moment please ', 'wait minute please ', 'oh could ask you something please yes by all means ', \"many thanks for your help please don't mention it \", \"thanks for the information you're very welcome \", 'thanks for helping me not at all ', \"thank you don't mention it my pleasure \", \"I'm sorry that's all right \", 'here you are ', 'sorry can you repeat the number more slowly ', 'beg your pardon ', 'beg your pardon did hear you right ', \"there you are we're in fine mess now \", 'what did tell you ', 'there you are knew it all along ', 'all right ', 'fair enough ', \"don't need your money fair enough as you wish \", 'to say please nicely ', 'giving of Christmas presents ', \"come on children it's time for the presents \", \"well that's just great or terrific \", \"well there you are haven't told you \", 'the whole lot or mess ', 'this is pretty kettle of fish ', 'what fine mess ', 'weird and wonderful ', 'wonderfully macabre or scary ', 'scary and wonderful ', 'face and back printing ', 'perfecting ', 'already ', 'beauty knows no pain ', \"that's fine state of affairs \", 'best wishes ', 'fine words ', 'the nice thing about her ', 'to have nice time ', 'she has beautiful figure ', 'packet ', 'no mean feat ', 'the fair er sex dated ', \"everything that's or all that's new beautiful important \", 'the belles-lettres ', 'those were wonderful days ', \"that's nice way to behave \", 'what wonderful or great prospects ', 'enjoy yourselves for week ', 'what would you like my pretty one ', 'Beauty and the Beast ', 'www.lai.ar.tum.de ', 'www.brave-art.eu ', 'quero.at ', 'www.ibka.org ', 'www.volksoper.at ', 'www.hausderkunst.de ', 'www.sixpackfilm.com ', 'www.eblinger.at ', 'www.meranowinefestival.com ', 'www.casinotropez.com ']\n",
      "nashod: error: If neither 'pretokenized' or 'no_ssplit' option is enabled, the input to the TokenizerProcessor must be a string or a Document object.\n",
      "Schöne\n",
      "dep\n",
      "Münchnerin\n",
      "all transltions ['Munich ', 'of Munich after ', \"Munich's old town \", 'agreement ', 'treaty ', 'the Treaty of Munich ', 'to conclude an agreement ', 'to sign treaty ', 'inhabitant of Munich ', \"my wife's from Munich \", 'Munich beer ', 'the people from Munich ', 'the people living in Munich ', 'specialty from Munich ', 'the mayor of Munich ', \"my wife's from Munich \", 'www.baywastiftung.de ', 'www.tum.de ']\n",
      "nashod: error: If neither 'pretokenized' or 'no_ssplit' option is enabled, the input to the TokenizerProcessor must be a string or a Document object.\n",
      "Münchnerin\n",
      "punct\n",
      "\"\n",
      "nashod: error: Request exception can happen due to an api connection error. Please check your connection and try again\n",
      "\"\n",
      "obl\n",
      "2018\n",
      "nashod: error: 2018 --> text must be a valid text with maximum 5000 character, otherwise it cannot be translated\n",
      "2018\n",
      "root\n",
      "werden\n",
      "all transltions [\"he's just turned thirty or had his 30th birthday \", \"she'll be 80 tomorrow \", 'to get or grow or become old er older ', 'to be getting on ', 'to change ', 'to grow or become poor ', 'to get or become better ', 'to improve ', 'to turn or grow or go or become pale ', 'to pale ', 'to go or become blind ', 'to get or grow or become angry ', 'to get or become impertinent ', 'to grow or become happy ', 'to turn out well ', 'to go or get or grow or turn cold ', 'to get or grow or become ill ', 'to sicken ', 'to take ill or sick ', 'to grow or get or become tired ', 'to tire ', 'to get or grow or become rich ', 'to turn or grow or go red ', 'to redden ', 'to blush ', 'to flush ', 'to grow or get slim ', 'to slim down ', 'to turn or go bad ', 'to grow or get or become worse ', 'to worsen ', 'to go mad ', 'to awake ', 'to wake up ', 'sb feels or is getting better hot cold sick ', 'sb feels as if or though ', 'sb will turn out to be sth ', \"what's to become of you \", \"sb will get somewhere won't get anywhere in life \", 'love turned into hate ', 'sth will turn into something nothing will come of sth ', \"that won't come to anything \", \"nothing's going to come of that \", 'no chance or way ', 'to become sth ', 'to turn into sth ', 'to become certainty nightmare ', \"to be sb's undoing \", 'every day that God gives or grants us ', 'to turn sth into sth ', 'to turn out all right or OK ', \"it's slowly getting somewhere \", 'the project is coming on nicely ', 'get move on ', 'what on earth is going to happen or are we going to do when ', \"it'll turn out OK in the end \", 'to peg out ', 'sth is granted or accorded to sb ', 'sb is granted or accorded sth ', 'you shall have justice ', 'well never did ', \"don't believe it \", 'things are going to change ', 'it is going to get or become better ', 'it is getting dark cold late ', 'we can expect cold weather today ', \"she hasn't been heard of for many years \", 'sb feels or is getting better hot cold sick ', 'to become sth ', \"didn't want to be or become hero \", \"if that won't be or become hit \", 'to become fashionable fact reality ', 'to become father ', 'what do you want to be when you grow up ', 'to become doctor an astronaut teacher ', 'to be or come first second last ', 'to come in first second last ', \"what's that going to be \", 'to turn out well or OK badly ', \"it'll be riot \", 'the two will be going out with each other ', 'it is getting dark light ', 'night is falling ', 'summer is coming ', \"it's coming up to or it's nearly o'clock \", 'it is high time ', 'it will be ten years since ', 'it will soon be ', 'let there be light: and there was light ', 'sb sth would do have done sth ', 'should think that ', 'if sb sth did had done sth ', 'sb sth will shall do sth ', 'sb sth is going to do sth ', \"sb sth will shall not or won't shan't do sth \", 'sb sth is not going to do sth ', 'sb sth will have done sth ', 'sth is going to happen ', 'would or could sb please do sth ', 'sb sth will probably be doing have done sth ', \"it's probably getting on for o'clock \", \"you must know what you're doing \", \"we'll still have time to cook when do the shopping tomorrow \", 'she said she is or was living in Stuttgart ', \"he says he'll be coming tomorrow \", 'they claim they can beat us at chess ', \"you're not going to now are you \", 'to be done ', \"somebody's calling you \", 'you have been informed that ', 'sth will be done ', \"let's get some work done go to sleep now \", 'there was singing and dancing ', 'rumour had it that ', 'development ', 'to be in the making ', 'mature ', 'to become aware of sth ', 'to become clear to sb ', 'to become clear to sb ', \"to get sth clear in one's mind \", 'to transpire ', 'to come out ', 'to break ', 'it transpired that ', 'word has gone out that ', 'sth leaks out to sb ', 'to go viral ', 'to take effect ', 'to go wild ', 'to become legally incompetent ', 'to desert ', 'to fall silent ', 'to mature ', 'to become reality ', 'to become grandfather ', 'to be widowed ', 'to turn out all right ', 'to begin to wonder ', 'to melt ', 'to weaken ', 'to be dwindling away ', 'to be given the push or the sack ', 'to become like putty ', 'to become grandmother ', 'schanzenfest.blogsport.de ', 'www.sudabehmohafez.de ', 'www.swissfilms.ch ', 'mb.mercedes-benz.com ', '4-seasons.de ', 'www.spain-tenerife.com ', 'www.foliatec.com ', 'www.boesendorfer.com ', 'www.swissfilms.ch ', 'www.uni-oldenburg.de ', 'kindergartenarchenoah.at ', 'www.gondwana-collection.com ', 'www.bosch-stiftung.de ', 'schanzenfest.blogsport.de ', 'www.gds-online.de ', 'www.chopstick.at ', 'www.daimler.com ', 'www.inst.at ', 'www.titanick.de ', 'www.inhorgenta.com ', 'www.russische-partnerin.de ', 'www.impulstanz.com ', 'www.bellicon.com ', 'www.primedica.de ', 'www.justtravelous.com ', 'www.getdigital.de ', 'www.bellicon.com ', 'www.mps.mpg.de ']\n",
      "nashod: error: If neither 'pretokenized' or 'no_ssplit' option is enabled, the input to the TokenizerProcessor must be a string or a Document object.\n",
      "werden\n",
      "punct\n",
      "!\n",
      "nashod: error: Request exception can happen due to an api connection error. Please check your connection and try again\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "from deep_translator import PonsTranslator\n",
    "translator=PonsTranslator('de','en')\n",
    "input_text='\"Schöne Münchnerin\" 2018 werden!'\n",
    "docc = nlp(input_text)\n",
    "input_text=docc.sentences[0]\n",
    "for word in input_text.words:\n",
    "    print(word.deprel)\n",
    "    print(word.text)\n",
    "    try:\n",
    "        tt=translator.translate(word.text,return_all=True)\n",
    "        print('all transltions',tt)\n",
    "        translated_nlp=nlp_en(tt)\n",
    "        # break\n",
    "        for translated_sent in translated_nlp.sentences:\n",
    "                for w in translated_sent.words:\n",
    "                    print('translated_nlp',w.text)\n",
    "                    # print('lemma:',w.lemma)\n",
    "                    # print('in stop words',w.lemma in stop_words)\n",
    "    except Exception as e:\n",
    "        print('nashod: error:',e)\n",
    "        print(word.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['become',\n",
       " 'go',\n",
       " 'get',\n",
       " 'turn out',\n",
       " 'be',\n",
       " 'being',\n",
       " 'existence',\n",
       " 'essence',\n",
       " 'his',\n",
       " 'its']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# translator.translate('warden')\n",
    "from deep_translator import LingueeTranslator\n",
    "# import translators as ts\n",
    "# word = 'werden'\n",
    "# ts.bing(word,from_language='de',to_language='en',professional_field='common')\n",
    "translated_word = LingueeTranslator(source='de', target='en').translate(word,return_all=True)\n",
    "translated_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "schöne\n",
      "False\n",
      "translation: Lovely\n",
      "False\n",
      "w lemma: Lovely\n",
      " w.lemma.lower() not in stop_words: True\n",
      "w.text.lower() not in stop_words: True\n",
      "translated_lemma: ['Lovely']\n",
      "münchnerin\n",
      "False\n",
      "translation: Munich woman\n",
      "False\n",
      "w lemma: Munich\n",
      " w.lemma.lower() not in stop_words: True\n",
      "w.text.lower() not in stop_words: True\n",
      "w lemma: woman\n",
      " w.lemma.lower() not in stop_words: True\n",
      "w.text.lower() not in stop_words: True\n",
      "translated_lemma: ['Munich', 'woman']\n",
      "werden\n",
      "True\n",
      "translation: will\n",
      "True\n",
      "w lemma: wollen\n",
      " w.lemma.lower() not in stop_words: True\n",
      "w.text.lower() not in stop_words: False\n",
      "translated_lemma: []\n"
     ]
    }
   ],
   "source": [
    "german_stop_words=set(stopwords.words('german'))\n",
    "translated_lemma=[]\n",
    "for word in sent.words:\n",
    "    if not any(c.isalpha() for c in word.text):\n",
    "        continue\n",
    "    print(word.text.lower())\n",
    "    print(word.text.lower()  in german_stop_words)\n",
    "    translated_word=translator.translate(word.text) \n",
    "    print('translation:',translated_word)\n",
    "    print(translated_word in stop_words)\n",
    "    translated_nlp=nlp(translated_word)\n",
    "    for translated_sent in translated_nlp.sentences:\n",
    "        for w in translated_sent.words:\n",
    "            print('w lemma:',w.lemma)\n",
    "            print(' w.lemma.lower() not in stop_words:', w.lemma.lower() not in stop_words)\n",
    "            print('w.text.lower() not in stop_words:',w.text.lower() not in stop_words)\n",
    "    # translated_lemma.append()\n",
    "    translated_lemma = [w.lemma for translated_sent in translated_nlp.sentences \\\n",
    "                    for w in translated_sent.words\\\n",
    "                    if w.lemma.lower() not in stop_words and w.text.lower() not in stop_words ]\n",
    "    print('translated_lemma:',translated_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "translator = GoogleTranslator(source='de', target='en')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compare translation outputs\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def cosine_similarity(X,Y):\n",
    "        # tokenization\n",
    "        X_list = word_tokenize(X) \n",
    "        Y_list = word_tokenize(Y)\n",
    "        \n",
    "        # sw contains the list of stopwords\n",
    "        sw = stopwords.words('english') \n",
    "        l1 =[];l2 =[]\n",
    "        \n",
    "        # remove stop words from the string\n",
    "        X_set = {w for w in X_list if not w in sw} \n",
    "        Y_set = {w for w in Y_list if not w in sw}\n",
    "        \n",
    "        # form a set containing keywords of both strings \n",
    "        rvector = X_set.union(Y_set) \n",
    "        for w in rvector:\n",
    "            if w in X_set: l1.append(1) # create a vector\n",
    "            else: l1.append(0)\n",
    "            if w in Y_set: l2.append(1)\n",
    "            else: l2.append(0)\n",
    "        c = 0\n",
    "        \n",
    "        # cosine formula \n",
    "        for i in range(len(rvector)):\n",
    "                c+= l1[i]*l2[i]\n",
    "        cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "        # print(\"similarity: \", cosine)\n",
    "        \n",
    "        return(cosine)\n",
    "        \n",
    "constraints=open('newstest2019_deen_en_v2_lemma.constraint.json').readlines()\n",
    "sd_out=open('SD_out_deen_v5_alltags_beam10_SAT3.out').readlines()\n",
    "nd_out=open('SD_out_deen_neurologic_deocding.out').readlines()\n",
    "ops_out=open('newstest2019-deen-helsinki_out.txt').readlines()\n",
    "ref=open('newstest2019-deen-ref.txt').readlines()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "rows=[]\n",
    "for i,(cons,sd,nd,ops,re) in enumerate(zip(constraints,sd_out,nd_out,ops_out,ref)):\n",
    "    sd_tg_cosine=cosine_similarity(sd,re)\n",
    "    nd_tg_cosine=cosine_similarity(nd,re)\n",
    "    if sd == nd:\n",
    "        continue\n",
    "    row={\n",
    "        'ind':i,\n",
    "        'constrains':cons,\n",
    "        'structura_decoding':sd,\n",
    "        'neurological_decoding':nd,\n",
    "        'ops':ops,\n",
    "        'target':re,\n",
    "        'sd_tg_cosine':sd_tg_cosine,\n",
    "        'nd_tg_cosine':nd_tg_cosine,\n",
    "        'sd_nd_diff':sd_tg_cosine-nd_tg_cosine\n",
    "    }\n",
    "    rows.append(row)\n",
    "    if i==100:\n",
    "        break\n",
    "    \n",
    "df=pd.DataFrame(rows)\n",
    "df.to_csv('translation_out_100.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ind</th>\n",
       "      <th>constrains</th>\n",
       "      <th>structura_decoding</th>\n",
       "      <th>neurological_decoding</th>\n",
       "      <th>fairseq</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[[\"Munich\", \"root\"]]\\n</td>\n",
       "      <td>Beautiful Munich girl 2018: Beautiful Munich w...</td>\n",
       "      <td>Beautiful Munich girl 2018: Beautiful Munich w...</td>\n",
       "      <td>Beautiful Munich 2018: Beautiful Munich 2018 i...</td>\n",
       "      <td>The Beauty of Munich 2018: the Beauty of Munic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[[\"www.ecs-webhosting.de\", \"root\"], [\"04.05.20...</td>\n",
       "      <td>By az, updated on 04.\\n</td>\n",
       "      <td>By az, updated on 04.05.2018 at 11:11\\n</td>\n",
       "      <td>By az, updated on 04 / 05 / 2018 at 11: 11\\n</td>\n",
       "      <td>From A-Z, updated on 04/05/2018 at 11:11\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[[\"she\", \"nsubj\"], [\"to\", \"root\"], [\"want\", \"r...</td>\n",
       "      <td>Yeah, she wants...\\n</td>\n",
       "      <td>Yeah, she wants...\\n</td>\n",
       "      <td>Yes, she wants...\\n</td>\n",
       "      <td>Yes, she wants to...\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[[\"2018\", \"obl\"], [\"he's\", \"root\"], [\"just\", \"...</td>\n",
       "      <td>\"Beautiful Munich\" 2018 will be!\\n</td>\n",
       "      <td>\"Beautiful Munich\" 2018 will be!\\n</td>\n",
       "      <td>Become a \"beautiful Munich woman\" in 2018!\\n</td>\n",
       "      <td>to become \"The Beauty of Munich\" in 2018!\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[[\"afternoon\", \"obl\"], [\"to\", \"root\"], [\"wait\"...</td>\n",
       "      <td>In the afternoon, another surprise awaits our ...</td>\n",
       "      <td>In the afternoon, another surprise awaits our ...</td>\n",
       "      <td>In the afternoon another surprise awaits our c...</td>\n",
       "      <td>In the afternoon there is another surprise wai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ind                                         constrains  \\\n",
       "0    0                             [[\"Munich\", \"root\"]]\\n   \n",
       "1    1  [[\"www.ecs-webhosting.de\", \"root\"], [\"04.05.20...   \n",
       "2    2  [[\"she\", \"nsubj\"], [\"to\", \"root\"], [\"want\", \"r...   \n",
       "3    3  [[\"2018\", \"obl\"], [\"he's\", \"root\"], [\"just\", \"...   \n",
       "4    4  [[\"afternoon\", \"obl\"], [\"to\", \"root\"], [\"wait\"...   \n",
       "\n",
       "                                  structura_decoding  \\\n",
       "0  Beautiful Munich girl 2018: Beautiful Munich w...   \n",
       "1                            By az, updated on 04.\\n   \n",
       "2                               Yeah, she wants...\\n   \n",
       "3                 \"Beautiful Munich\" 2018 will be!\\n   \n",
       "4  In the afternoon, another surprise awaits our ...   \n",
       "\n",
       "                               neurological_decoding  \\\n",
       "0  Beautiful Munich girl 2018: Beautiful Munich w...   \n",
       "1            By az, updated on 04.05.2018 at 11:11\\n   \n",
       "2                               Yeah, she wants...\\n   \n",
       "3                 \"Beautiful Munich\" 2018 will be!\\n   \n",
       "4  In the afternoon, another surprise awaits our ...   \n",
       "\n",
       "                                             fairseq  \\\n",
       "0  Beautiful Munich 2018: Beautiful Munich 2018 i...   \n",
       "1       By az, updated on 04 / 05 / 2018 at 11: 11\\n   \n",
       "2                                Yes, she wants...\\n   \n",
       "3       Become a \"beautiful Munich woman\" in 2018!\\n   \n",
       "4  In the afternoon another surprise awaits our c...   \n",
       "\n",
       "                                              target  \n",
       "0  The Beauty of Munich 2018: the Beauty of Munic...  \n",
       "1         From A-Z, updated on 04/05/2018 at 11:11\\n  \n",
       "2                             Yes, she wants to...\\n  \n",
       "3        to become \"The Beauty of Munich\" in 2018!\\n  \n",
       "4  In the afternoon there is another surprise wai...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples=$HOME/.mt-metrics-eval/mt-metrics-eval/wmt20/metric-scores/en-de\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  [\n",
       "    {\n",
       "      \"id\": 1,\n",
       "      \"text\": \"John\",\n",
       "      \"lemma\": \"John\",\n",
       "      \"upos\": \"PROPN\",\n",
       "      \"xpos\": \"NNP\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 2,\n",
       "      \"deprel\": \"nsubj\",\n",
       "      \"start_char\": 0,\n",
       "      \"end_char\": 4\n",
       "    },\n",
       "    {\n",
       "      \"id\": 2,\n",
       "      \"text\": \"introduced\",\n",
       "      \"lemma\": \"introduce\",\n",
       "      \"upos\": \"VERB\",\n",
       "      \"xpos\": \"VBD\",\n",
       "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin\",\n",
       "      \"head\": 0,\n",
       "      \"deprel\": \"root\",\n",
       "      \"start_char\": 5,\n",
       "      \"end_char\": 15\n",
       "    },\n",
       "    {\n",
       "      \"id\": 3,\n",
       "      \"text\": \"Kayla\",\n",
       "      \"lemma\": \"Kayla\",\n",
       "      \"upos\": \"PROPN\",\n",
       "      \"xpos\": \"NNP\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 2,\n",
       "      \"deprel\": \"obj\",\n",
       "      \"start_char\": 16,\n",
       "      \"end_char\": 21\n",
       "    },\n",
       "    {\n",
       "      \"id\": 4,\n",
       "      \"text\": \"to\",\n",
       "      \"lemma\": \"to\",\n",
       "      \"upos\": \"ADP\",\n",
       "      \"xpos\": \"IN\",\n",
       "      \"head\": 5,\n",
       "      \"deprel\": \"case\",\n",
       "      \"start_char\": 22,\n",
       "      \"end_char\": 24\n",
       "    },\n",
       "    {\n",
       "      \"id\": 5,\n",
       "      \"text\": \"Tim\",\n",
       "      \"lemma\": \"Tim\",\n",
       "      \"upos\": \"PROPN\",\n",
       "      \"xpos\": \"NNP\",\n",
       "      \"feats\": \"Number=Sing\",\n",
       "      \"head\": 2,\n",
       "      \"deprel\": \"obl\",\n",
       "      \"start_char\": 25,\n",
       "      \"end_char\": 28\n",
       "    },\n",
       "    {\n",
       "      \"id\": 6,\n",
       "      \"text\": \".\",\n",
       "      \"lemma\": \".\",\n",
       "      \"upos\": \"PUNCT\",\n",
       "      \"xpos\": \".\",\n",
       "      \"head\": 2,\n",
       "      \"deprel\": \"punct\",\n",
       "      \"start_char\": 28,\n",
       "      \"end_char\": 29\n",
       "    }\n",
       "  ]\n",
       "]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_en('John introduced Kayla to Tim.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('hug3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a9dae2ba1a503386b4a9b9b06d7d17319e86cb4a528f7600682853ca5d224ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
