{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-large')\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-large')\n",
    "\n",
    "# input = \"My name is Azeem and I live in India\"\n",
    "# input = \"It has never been easy to have a rational conversation about the value of gold.\"\n",
    "input = \"The horse rides the astronaut. \"\n",
    "# You can also use \"translate English to French\" and \"translate English to Romanian\"\n",
    "input_ids = tokenizer(\"translate English to German: \"+input, return_tensors=\"pt\").input_ids  # Batch size 1\n",
    "\n",
    "outputs = model.generate(input_ids)\n",
    "\n",
    "decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert SGM to txt file\n",
    "import re\n",
    "path='./sgm/newstest2019-ende-src.en.sgm'#'./sgm/newstest2019-ende-ref.de.sgm'#'./sgm/newstest2019-deen-ref.en.sgm'\n",
    "text_out='newstest2019-ende-src.txt'#'newstest2019-ende-ref.txt'#'newstest2019-deen-ref.txt'\n",
    "seg_re = re.compile(r\"<seg id=\\\"\\d+\\\">(.*)</seg>\")\n",
    "with open(text_out,'w', encoding=\"utf-8\") as g:\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            seg_match = re.match(seg_re, line)\n",
    "            if seg_match:\n",
    "                assert len(seg_match.groups()) == 1\n",
    "            else:\n",
    "                continue\n",
    "            out_line= seg_match.groups()[0]\n",
    "            g.write(out_line)\n",
    "            g.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-18 18:53:30.248808: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'FSMTForConditionalGeneration' from 'transformers' (/home/mbastan/anaconda3/envs/hug3/lib/python3.7/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2259207/4169044909.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m## Translate txt from de to en\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFSMTForConditionalGeneration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFSMTTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"facebook/wmt19-en-de\"\u001b[0m\u001b[0;31m#\"facebook/wmt19-de-en\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFSMTTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFSMTForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'FSMTForConditionalGeneration' from 'transformers' (/home/mbastan/anaconda3/envs/hug3/lib/python3.7/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "## Translate txt from de to en\n",
    "# new_t = '/home/mbastan/.local/lib_old/python3.9_old/site-packages/transformers'\n",
    "from transformers import FSMTForConditionalGeneration, FSMTTokenizer\n",
    "mname = \"facebook/wmt19-en-de\"#\"facebook/wmt19-de-en\"\n",
    "tokenizer = FSMTTokenizer.from_pretrained(mname)\n",
    "model = FSMTForConditionalGeneration.from_pretrained(mname)\n",
    "fair_out_path = 'newstest2019-ende-out.txt'\n",
    "with open(text_out, encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "with open(fair_out_path,'w', encoding=\"utf-8\") as g:\n",
    "    for input in lines:\n",
    "        input_ids = tokenizer.encode(input.strip(), return_tensors=\"pt\")\n",
    "        outputs = model.generate(input_ids)\n",
    "        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # print(decoded)\n",
    "        g.write(decoded)\n",
    "        g.write('\\n')\n",
    "        #  # Machine learning is great, isn't it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## evaluate the output\n",
    "from sacrebleu import corpus_bleu\n",
    "def calculate_bleu_score(output_lns, refs_lns, score_path):\n",
    "    bleu = corpus_bleu(output_lns, [refs_lns])\n",
    "    result = \"BLEU score: {}\".format(bleu.score)\n",
    "    with open(score_path,\"w\") as score_file:\n",
    "        score_file.write(result)\n",
    "with open('newstest2019-deen-out.txt') as output:\n",
    "    output_lns=output.readlines()\n",
    "with open('newstest2019-deen-ref.txt') as reflns:\n",
    "    refs_lns=reflns.readlines()\n",
    "    \n",
    "calculate_bleu_score(output_lns, refs_lns, './blueScore')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-18 10:53:55 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json: 193kB [00:00, 52.0MB/s]                    \n",
      "2023-01-18 10:53:55 WARNING: Language de package default expects mwt, which has been added\n",
      "2023-01-18 10:53:56 INFO: Loading these models for language: de (German):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | gsd     |\n",
      "| mwt       | gsd     |\n",
      "| pos       | gsd     |\n",
      "| lemma     | gsd     |\n",
      "| depparse  | gsd     |\n",
      "=======================\n",
      "\n",
      "2023-01-18 10:53:56 INFO: Use device: gpu\n",
      "2023-01-18 10:53:56 INFO: Loading: tokenize\n",
      "2023-01-18 10:53:56 INFO: Loading: mwt\n",
      "2023-01-18 10:53:56 INFO: Loading: pos\n",
      "2023-01-18 10:53:56 INFO: Loading: lemma\n",
      "2023-01-18 10:53:56 INFO: Loading: depparse\n",
      "2023-01-18 10:53:56 INFO: Done loading processors!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "unsupported format character '_' (0x5f) at index 18",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2259207/3338769306.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stanza_documents_%_de.pk'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mpkname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mstanza_documents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unsupported format character '_' (0x5f) at index 18",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2259207/3338769306.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mstanza_documents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m## save the documents on a file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stanza_documents_%_de.pk'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mpkname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstanza_documents\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: unsupported format character '_' (0x5f) at index 18"
     ]
    }
   ],
   "source": [
    "## extract structures\n",
    "import stanza\n",
    "from typing import List\n",
    "from stanza_batch import batch\n",
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "path1='newstest2019-deen-in.txt'\n",
    "# path1='/home/mbastan/context_home/structuralDecoding/a_star_neurologic/dataset/machine_translation/newstest2017-iate/iate.414.terminology.tsv.de'\n",
    "\n",
    "pkname='newstest2019'\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\"\n",
    "nlp = stanza.Pipeline('de', processors='lemma,tokenize,pos,depparse',use_gpu=True)\n",
    "with open(path1) as f:\n",
    "    lines = f.readlines()\n",
    "try:\n",
    "    with open('stanza_documents_%_de.pk'%pkname,'rb') as f:\n",
    "        stanza_documents=pickle.load(f)\n",
    "except:\n",
    "    ## batch document processing to increase the speed\n",
    "    stanza_documents: List[stanza.Document] = []\n",
    "    for document in batch(lines, nlp, batch_size=64): # Default batch size is 32\n",
    "        stanza_documents.append(document)\n",
    "    ## save the documents on a file\n",
    "    with open('stanza_documents_%_de.pk'%pkname,'wb') as f:\n",
    "        pickle.dump(stanza_documents,f)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import PonsTranslator\n",
    "translator = PonsTranslator(source='de', target='en')\n",
    "outpath='newstest2019_deen_en.constraint.json' #'iate.414_en.terminology.constraint.json'\n",
    "with open(outpath,'w') as f:\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for doc in  stanza_documents:\n",
    "        sent = doc.sentences[0]\n",
    "        arr_all = []\n",
    "        arr_ori = []\n",
    "        arr_rev = []\n",
    "        arr_lex =[]\n",
    "        sent_map ={}\n",
    "        for word in sent.words:\n",
    "            arr_all.append([word.text,word.deprel])\n",
    "            # if word.deprel in sent_map:\n",
    "            #     continue\n",
    "            if word.deprel == 'nsubj' or word.deprel == 'obj' or word.deprel == 'root' or word.deprel == 'obl':\n",
    "                if( word.deprel == 'obl' and 'obj' in sent_map) or ( word.deprel == 'obj' and 'obl' in sent_map):\n",
    "                    continue\n",
    "                \n",
    "                # ids = tokenizer.encode(word.text.strip(), return_tensors=\"pt\")\n",
    "                # outs = model.generate(ids,do_sample=True, top_k=30, top_p=0.95)\n",
    "                # decoded = tokenizer.decode(outs[0], skip_special_tokens=True)\n",
    "                try:\n",
    "                    translated_word=translator.translate(word.text)\n",
    "                except:\n",
    "                    translated_word=word.text\n",
    "                translated_nlp=nlp(translated_word)\n",
    "                translated_lemma = [w.lemma for translated_sent in translated_nlp.sentences \\\n",
    "                    for w in translated_sent.words\\\n",
    "                        if w.lemma.lower() not in stop_words and w.text not in stop_words]\n",
    "                for tlemma in translated_lemma:                \n",
    "                    arr_ori.append([tlemma,word.deprel])\n",
    "\n",
    "\n",
    "        ## write all structures\n",
    "        json.dump(arr_ori,f)\n",
    "        f.write('\\n')\n",
    "        \n",
    "        # json.dump(arr_ori,g_ori)\n",
    "        # g_ori.write('\\n')\n",
    "        \n",
    "        # json.dump(arr_rev,g_rev)\n",
    "        # g_rev.write('\\n')\n",
    "        \n",
    "        # json.dump(arr_lex,f_lexica_constraints)\n",
    "        # f_lexica_constraints.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compare translation outputs\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def cosine_similarity(X,Y):\n",
    "        # tokenization\n",
    "        X_list = word_tokenize(X) \n",
    "        Y_list = word_tokenize(Y)\n",
    "        \n",
    "        # sw contains the list of stopwords\n",
    "        sw = stopwords.words('english') \n",
    "        l1 =[];l2 =[]\n",
    "        \n",
    "        # remove stop words from the string\n",
    "        X_set = {w for w in X_list if not w in sw} \n",
    "        Y_set = {w for w in Y_list if not w in sw}\n",
    "        \n",
    "        # form a set containing keywords of both strings \n",
    "        rvector = X_set.union(Y_set) \n",
    "        for w in rvector:\n",
    "            if w in X_set: l1.append(1) # create a vector\n",
    "            else: l1.append(0)\n",
    "            if w in Y_set: l2.append(1)\n",
    "            else: l2.append(0)\n",
    "        c = 0\n",
    "        \n",
    "        # cosine formula \n",
    "        for i in range(len(rvector)):\n",
    "                c+= l1[i]*l2[i]\n",
    "        cosine = c / float((sum(l1)*sum(l2))**0.5)\n",
    "        # print(\"similarity: \", cosine)\n",
    "        \n",
    "        return(cosine)\n",
    "        \n",
    "constraints=open('newstest2019_deen.constraint.json').readlines()\n",
    "sd_out=open('SD_out_deen_v3.out').readlines()\n",
    "nd_out=open('SD_out_deen_neurologic_deocding.out').readlines()\n",
    "fairseq_out=open('newstest2019-deen-out.txt').readlines()\n",
    "ref=open('newstest2019-deen-ref.txt').readlines()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "rows=[]\n",
    "for i,(cons,sd,nd,fair,re) in enumerate(zip(constraints,sd_out,nd_out,fairseq_out,ref)):\n",
    "    sd_nd_cosine=cosine_similarity(sd,nd)\n",
    "    sd_re_cosine=cosine_similarity(sd,re)\n",
    "    row={\n",
    "        'ind':i,\n",
    "        'constrains':cons,\n",
    "        'structura_decoding':sd,\n",
    "        'neurological_decoding':nd,\n",
    "        'fairseq':fair,\n",
    "        'target':re,\n",
    "        'sd_nd_cosine':sd_nd_cosine,\n",
    "        'sd_re_cosine':sd_re_cosine,\n",
    "        'sd_nd_diff':sd_re_cosine-sd_nd_cosine\n",
    "    }\n",
    "    rows.append(row)\n",
    "    \n",
    "df=pd.DataFrame(rows)\n",
    "df.to_csv('translation_out.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ind</th>\n",
       "      <th>constrains</th>\n",
       "      <th>structura_decoding</th>\n",
       "      <th>neurological_decoding</th>\n",
       "      <th>fairseq</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[[\"Munich\", \"root\"]]\\n</td>\n",
       "      <td>Beautiful Munich girl 2018: Beautiful Munich w...</td>\n",
       "      <td>Beautiful Munich girl 2018: Beautiful Munich w...</td>\n",
       "      <td>Beautiful Munich 2018: Beautiful Munich 2018 i...</td>\n",
       "      <td>The Beauty of Munich 2018: the Beauty of Munic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[[\"www.ecs-webhosting.de\", \"root\"], [\"04.05.20...</td>\n",
       "      <td>By az, updated on 04.\\n</td>\n",
       "      <td>By az, updated on 04.05.2018 at 11:11\\n</td>\n",
       "      <td>By az, updated on 04 / 05 / 2018 at 11: 11\\n</td>\n",
       "      <td>From A-Z, updated on 04/05/2018 at 11:11\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[[\"she\", \"nsubj\"], [\"to\", \"root\"], [\"want\", \"r...</td>\n",
       "      <td>Yeah, she wants...\\n</td>\n",
       "      <td>Yeah, she wants...\\n</td>\n",
       "      <td>Yes, she wants...\\n</td>\n",
       "      <td>Yes, she wants to...\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[[\"2018\", \"obl\"], [\"he's\", \"root\"], [\"just\", \"...</td>\n",
       "      <td>\"Beautiful Munich\" 2018 will be!\\n</td>\n",
       "      <td>\"Beautiful Munich\" 2018 will be!\\n</td>\n",
       "      <td>Become a \"beautiful Munich woman\" in 2018!\\n</td>\n",
       "      <td>to become \"The Beauty of Munich\" in 2018!\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[[\"afternoon\", \"obl\"], [\"to\", \"root\"], [\"wait\"...</td>\n",
       "      <td>In the afternoon, another surprise awaits our ...</td>\n",
       "      <td>In the afternoon, another surprise awaits our ...</td>\n",
       "      <td>In the afternoon another surprise awaits our c...</td>\n",
       "      <td>In the afternoon there is another surprise wai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ind                                         constrains  \\\n",
       "0    0                             [[\"Munich\", \"root\"]]\\n   \n",
       "1    1  [[\"www.ecs-webhosting.de\", \"root\"], [\"04.05.20...   \n",
       "2    2  [[\"she\", \"nsubj\"], [\"to\", \"root\"], [\"want\", \"r...   \n",
       "3    3  [[\"2018\", \"obl\"], [\"he's\", \"root\"], [\"just\", \"...   \n",
       "4    4  [[\"afternoon\", \"obl\"], [\"to\", \"root\"], [\"wait\"...   \n",
       "\n",
       "                                  structura_decoding  \\\n",
       "0  Beautiful Munich girl 2018: Beautiful Munich w...   \n",
       "1                            By az, updated on 04.\\n   \n",
       "2                               Yeah, she wants...\\n   \n",
       "3                 \"Beautiful Munich\" 2018 will be!\\n   \n",
       "4  In the afternoon, another surprise awaits our ...   \n",
       "\n",
       "                               neurological_decoding  \\\n",
       "0  Beautiful Munich girl 2018: Beautiful Munich w...   \n",
       "1            By az, updated on 04.05.2018 at 11:11\\n   \n",
       "2                               Yeah, she wants...\\n   \n",
       "3                 \"Beautiful Munich\" 2018 will be!\\n   \n",
       "4  In the afternoon, another surprise awaits our ...   \n",
       "\n",
       "                                             fairseq  \\\n",
       "0  Beautiful Munich 2018: Beautiful Munich 2018 i...   \n",
       "1       By az, updated on 04 / 05 / 2018 at 11: 11\\n   \n",
       "2                                Yes, she wants...\\n   \n",
       "3       Become a \"beautiful Munich woman\" in 2018!\\n   \n",
       "4  In the afternoon another surprise awaits our c...   \n",
       "\n",
       "                                              target  \n",
       "0  The Beauty of Munich 2018: the Beauty of Munic...  \n",
       "1         From A-Z, updated on 04/05/2018 at 11:11\\n  \n",
       "2                             Yes, she wants to...\\n  \n",
       "3        to become \"The Beauty of Munich\" in 2018!\\n  \n",
       "4  In the afternoon there is another surprise wai...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('hug3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0a9dae2ba1a503386b4a9b9b06d7d17319e86cb4a528f7600682853ca5d224ae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
